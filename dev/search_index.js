var documenterSearchIndex = {"docs":
[{"location":"","page":"Home","title":"Home","text":"CurrentModule = DynamicDiscreteChoice","category":"page"},{"location":"#DynamicDiscreteChoice","page":"Home","title":"DynamicDiscreteChoice","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for DynamicDiscreteChoice.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [DynamicDiscreteChoice]","category":"page"},{"location":"#Base.:*-Tuple{MarkovChain, MarkovChain}","page":"Home","title":"Base.:*","text":"*(c1::MarkovChain, c2::MarkovChain)\n\nAssuming c1 and c2 are independent, create a new MarkovChain representing the evolution of the combination of states in c1 and c2.\n\n\n\n\n\n","category":"method"},{"location":"#DynamicDiscreteChoice.bootstrap_series-NTuple{4, Any}","page":"Home","title":"DynamicDiscreteChoice.bootstrap_series","text":"bootstrap_series(payoffs,v,T,ddc; B=999)\n\nUse the parametric bootstrap for inference for a dynamic discrete choice model.  Data is assumed to come from a single time series of length T.\n\nInputs:\n\npayoffs estimated payoffs\nv estimated choice specific value functions\nT length of data \nddc DDC model containing estimated transitions in ddc.transition\nB number of bootstrap replications\n\n\n\n\n\n","category":"method"},{"location":"#DynamicDiscreteChoice.bootstrap_table-Tuple{Any, Any}","page":"Home","title":"DynamicDiscreteChoice.bootstrap_table","text":"bootstrap_table(θ,θb;coverage=0.95)\n\nGiven a matrix of parameter estimates, θ, and a vector of matrices of  bootstrap replicants, θb, returns an array with rows alternating between  rows of θ and rows of tuples containing confidence intervals with   coverage coverage probability.\n\n\n\n\n\n","category":"method"},{"location":"#DynamicDiscreteChoice.choicep-Tuple{Any, Any}","page":"Home","title":"DynamicDiscreteChoice.choicep","text":"choicep(v, ddc)\n\nGiven choice specific value functions, returns conditional choice probabilities.\n\n\n\n\n\n","category":"method"},{"location":"#DynamicDiscreteChoice.choicespecificvalue!-Tuple{Any, Any, Any}","page":"Home","title":"DynamicDiscreteChoice.choicespecificvalue!","text":"choicespecificvalue!(v, V,ddc)\n\nGiven a dynamic discrete choice model, ddc, and exante value function V, sets v equal to the choice specific value functions.\n\nvas = uas + δ EV(s)  a s\n\n\n\n\n\n","category":"method"},{"location":"#DynamicDiscreteChoice.emax","page":"Home","title":"DynamicDiscreteChoice.emax","text":"emax(v, d=Distributions.Gumbel())\n\nReturns E[maxᵢ v[i] + ϵ[i] ] where ϵ[i] are i.i.d. d\n\n\n\n\n\n","category":"function"},{"location":"#DynamicDiscreteChoice.emaxp","page":"Home","title":"DynamicDiscreteChoice.emaxp","text":"emaxp(p, i0=1, d=Distributions.Gumbel())\n\nReturns E[maxᵢ v[i] - v[i0] + ϵ[i] ] where ϵ[i] are i.i.d. d and p[i] = P(i = argmaxⱼ v[j] - v[i0] + ϵ[j])\n\n\n\n\n\n","category":"function"},{"location":"#DynamicDiscreteChoice.estimate-Tuple{Any, AbstractArray, DynamicDiscreteChoice.DDC}","page":"Home","title":"DynamicDiscreteChoice.estimate","text":"estimate(p, transition, ddc::DDC; zero_action=first(ddc.actions)[1])\n\nEstimate payoffs of dynamic discrete choice model. p should be estimates of the conditional choice probabilities. transition should be a \"states\" by \"states\" by \"actions\" array, with transition[new,old,action]=P(s=new|s=old,a=action).\n\n\n\n\n\n","category":"method"},{"location":"#DynamicDiscreteChoice.estimate-Tuple{Any, ControlledMarkovChain, DynamicDiscreteChoice.DDC}","page":"Home","title":"DynamicDiscreteChoice.estimate","text":"estimate(p, transition, ddc::DDC; zero_action=first(ddc.actions)[1])\n\nEstimate payoffs of dynamic discrete choice model. p should be estimates of the conditional choice probabilities. transition should be a ControlledMarkovChain representing the transitions when action a is chosen.\n\n\n\n\n\n","category":"method"},{"location":"#DynamicDiscreteChoice.estimate_series-Tuple{Any, Any, Any}","page":"Home","title":"DynamicDiscreteChoice.estimate_series","text":"estimate_series(action_data, state_data, discount; zero_action=action_data[1],\n                Fϵ=Distributions.Gumbel(),\n                actions = OrderedDict(a => i for (i,a) ∈ enumerate(unique(action_data))),\n                states = OrderedDict(s => i for (i,s) ∈ enumerate(unique(state_data))))\n\nGiven a vector of observed actions, action_data, and states, state_data, and discount rate, computes an estimate of the payoffs of the associated dynamic discrete choice model.\n\nNormalizes the payoff of zero_action to 0 in all states.\n\naction_data and state_data should be single time-series realizations of the game.  Conditional sample means are used to estimate conditional choice probabilities and transition probabilities. Use estimate(p, ddc::DDC; zero_action=first(ddc.actions)[1]) if you want to use other estimates for choice and transition probabilities.\n\n\n\n\n\n","category":"method"},{"location":"#DynamicDiscreteChoice.exante_bellman-Tuple{Any, Any, Any}","page":"Home","title":"DynamicDiscreteChoice.exante_bellman","text":"exante_bellman(V,v, ddc)\n\nCompute the exante (before ϵ is known) Bellman operator for dynamic discrete choice model ddc. i.e. returns\n\nVs = Emaxₐuas + ϵa + δ EVs  a s \n\n\n\n\n\n","category":"method"},{"location":"#DynamicDiscreteChoice.hotzmiller","page":"Home","title":"DynamicDiscreteChoice.hotzmiller","text":"hotzmiller(p, a0, d::Distributions.Gumbel=Distributions.Gumbel())\n\nGiven choice probabilities, return differences of choice specific value functions.\n\n\n\n\n\n","category":"function"},{"location":"#DynamicDiscreteChoice.simulate-Tuple{Any, Any, Any}","page":"Home","title":"DynamicDiscreteChoice.simulate","text":"simulate(T, ddc, v)\n\nSimulates dynamic discrete choice problem for T periods with choice specific value functions v.\n\nReturns a named tuple consisting of states, state indices, actions, and action indices.\n\n\n\n\n\n","category":"method"},{"location":"#DynamicDiscreteChoice.simulate-Tuple{Any, Any}","page":"Home","title":"DynamicDiscreteChoice.simulate","text":"simulate(T, ddc)\n\nSimulates dynamic discrete choice problem for T periods.\n\nReturns a named tuple consisting of states, state indices, actions, and action indices.\n\n\n\n\n\n","category":"method"},{"location":"#DynamicDiscreteChoice.value-Tuple{Any}","page":"Home","title":"DynamicDiscreteChoice.value","text":"value(ddc; kwargs...)\n\nCompute the value function for the dynamic discrete choice model ddc.\n\nNLsolve.fixedpoint is used to compute the value functions. kwargs are options passed to NLsolve.fixedpoint.\n\nReturns a name tuple containing the value function at each state, V, the choice specific value function, v, and the output of NLsolve.fixedpoint, solver_output.\n\n\n\n\n\n","category":"method"}]
}
